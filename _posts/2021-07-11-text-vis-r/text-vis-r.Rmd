---
title: "Text Visual Analytics with R"
description: |
  This in-class exercise explores the concepts and methods of Text Visualisation. It visualises and analyses the text data from a collection of 20 newsgroups. It introduces the tidytext framework for processing, wrangling, analysing and visualising text data using tidytext, tidyverse, widyr, wordcloud, ggwordcloud, textplot, DT, lubridate, and hms packages.

preview: preview_image.png  
author:
  - name: Archie Dolit
    url: https://www.linkedin.com/in/adolit/
    affiliation: School of Computing and Information Systems, Singapore Management University
    affiliation_url: https://scis.smu.edu.sg/
date: 07-11-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
categories:
  - Text Visualisation
  - R
  - In-class Exercise
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina=3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

## Install and Lauch R Packages

```{r r package}
packages = c('tidytext', 
             'widyr', 'wordcloud',
             'DT', 'ggwordcloud', 
             'textplot', 'lubridate', 
             'hms','tidyverse', 
             'tidygraph', 'ggraph',
             'igraph')
for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

## Import Multiple Text Files from Multiple Folders

#### Step 1: Creating a folder list

```{r folder}
news20 <- "data/20news/"
```

#### Step 2: Define a function to read all files from a folder into a data frame

```{r read folder func}
read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, 
                      read_lines)) %>%
    transmute(id = basename(file), 
              text) %>%
    unnest(text)
}
```

#### Step 3: Reading in all the messages from the 20news folder

```{r read all}
raw_text <- tibble(folder = 
                     dir(news20, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), 
            id, text)
write_rds(raw_text, "data/rds/news20.rds")
```

* [*read_lines()*](https://readr.tidyverse.org/reference/read_lines.html) of [**readr**](https://readr.tidyverse.org/index.html) package is used to read up to n_max lines from a file.

* [*map()*](https://purrr.tidyverse.org/reference/map.html) of [**purrr**](https://purrr.tidyverse.org/index.html) package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.

* [*unnest()*](https://tidyr.tidyverse.org/reference/nest.html) of **dplyr** package is used to flatten a list-column of data frames back out into regular columns.

* [mutate()](https://dplyr.tidyverse.org/reference/mutate.html) of **dplyr** is used to add new variables and preserves existing ones; 

* [transmute()](https://dplyr.tidyverse.org/reference/mutate.html) of **dplyr** is used to add new variables and drops existing ones.

* [read_rds()](https://readr.tidyverse.org/reference/read_rds.html) is used to save the extracted and combined data frame as rds file for future use.

## Initial Explorartory Analysis

Frequency of messages by newsgroup

```{r EDA}
raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
```

## Cleaning Text Data

#### Step 1: Removing header and automated email signatures

Notice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like "--".

```{r text cleaning 1}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()
```

* [*cumsum()*](https://rdrr.io/r/base/cumsum.html) of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.  

* [*str_detect()*](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.

#### Step 2: Removing lines with nested text representing quotes from other users. 

Regular expressions are used to remove with nested text representing quotes from other users.

```{r text cleaning 2}
cleaned_text <- cleaned_text %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )
```

* [*str_detect()*](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.

* [*filter()*](https://dplyr.tidyverse.org/reference/filter.html) of **dplyr** package is used to subset a data frame, retaining all rows that satisfy the specified conditions.


## Text Data Processing

[*unnest_tokens()*](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens, while [*stop_words()*](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words.

```{r text processing}
usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)
```

Find the most common words in the entire dataset, or within particular newsgroups

```{r common words}
usenet_words %>%
  count(word, sort = TRUE)
```
Instead of counting individual word, you can also count words within by newsgroup

```{r newsgroup common words}
words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>%
  ungroup()
```


## Visualising Words in newsgroups

*wordcloud()* of **wordcloud** package is used to plot a static wordcloud

```{r static wordcloud}
wordcloud(words_by_newsgroup$word,
          words_by_newsgroup$n,
          max.words = 300)
```
A DT table can be used to complement the visual discovery.

```{r dt table}
DT::datatable(words_by_newsgroup, 
              filter = 'top') %>% 
  formatStyle(0, target = 'row', 
              lineHeight='25%')
```
The wordcloud below is plotted by using [**ggwordcloud**](https://lepennec.github.io/ggwordcloud/) package.

```{r ggwordcloud}
set.seed(1234)
words_by_newsgroup %>%
  filter(n > 5) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```
### Computing tf-idf within newsgroups

*bind_tf_idf()* of tidytext is used to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.   

```{r}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))
```

## Visualising tf-idf as interactive table

*datatable()* of DT package to create a html table that allows pagination of rows and columns

```{r dt table tf_idf}
DT::datatable(tf_idf, filter = 'top') %>% 
  formatRound(columns = c('tf', 'idf', 
                          'tf_idf'), 
              digits = 3) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='25%')
```
* *filter* argument is used to turn control the filter UI.

* *formatRound()* is used to customise the values format.  The argument *digits* define the number of decimal places.

* *formatStyle()* is used to customise the output table.  In this example, the arguments *target* and *lineHeight* are used to reduce the line height by 25%.

## Visualising tf-idf within newsgroups

Facet bar charts technique is used to visualise the tf-idf values of science related newsgroup.

```{r facet bar charts}
tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

## Counting and correlating pairs of words with the widyr package

[**widyr**](https://cran.r-project.org/web/packages/widyr/index.html) package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it,
then re-tidies the result. 

*pairwise_cor()* of **widyr** package is used to compute the correlation between newsgroup based on the common words found.

```{r pairwise_cor}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, 
               word, 
               n, 
               sort = TRUE)
```


## Visualising correlation as a network

Visualise the relationship between newgroups in network graph

```{r visualising correlation}
set.seed(2017)

newsgroup_cors %>%
  filter(correlation > .025) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "lightblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```

## Bigram

Bigram data frame is created by using unnest_tokens() of tidytext

```{r bigram}
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)

bigrams
```

## Counting bigrams

Count and sort the bigram data frame in ascending order

```{r bigram count}
bigrams_count <- bigrams %>%
  filter(bigram != 'NA') %>%
  count(bigram, sort = TRUE)

bigrams_count
```

## Cleaning bigram

Separate the bigram into two words

```{r separate bigram}
bigrams_separated <- bigrams %>%
  filter(bigram != 'NA') %>%
  separate(bigram, c("word1", "word2"), 
           sep = " ")
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_filtered
```

## Counting the bigram again

```{r bigram count again}
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```

## Network graph from bigram data frame

Network graph is created by using *graph_from_data_frame()* of igraph. 

```{r network graph}
bigram_graph <- bigram_counts %>%
  filter(n > 3) %>%
  graph_from_data_frame()
bigram_graph
```

## Visualizing a network of bigrams with ggraph

ggraph package is used to plot the bigram.

```{r visualise bigram}
set.seed(1234)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), 
                 vjust = 1, 
                 hjust = 1)
```
## Revised version

```{r revised visualise bigram}
set.seed(1234)

a <- grid::arrow(type = "closed", 
                 length = unit(.15,
                               "inches"))
ggraph(bigram_graph, 
       layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), 
                 show.legend = FALSE,
                 arrow = a, 
                 end_cap = circle(.07,
                                  'inches')) +
  geom_node_point(color = "lightblue", 
                  size = 5) +
  geom_node_text(aes(label = name), 
                 vjust = 1, 
                 hjust = 1) +
  theme_void()
```

## Reference:
* [Lesson 10: Text Visualisation in R In-Class Exercise](https://isss608.netlify.app/in-class_ex/in-class_ex10/in-class_ex10-textva#1)