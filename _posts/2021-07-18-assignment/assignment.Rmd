---
title: "Visual Detective Assignment"
description: |
  This assignment attempts to solve the 2021 IEEE Visual Analytics Science and Technology (VAST) Challenge: Mini-Challenge 2 by applying different visual analytics concepts, methods, and techniques with relevant R data visualisation and data analysis packages.

preview: img/preview_image.png  
author:
  - name: Archie Dolit
    url: https://www.linkedin.com/in/adolit/
    affiliation: School of Computing and Information Systems, Singapore Management University
date: 07-25-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 3
categories:
  - Visual Detective
  - R
  - Assignment
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina=3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# 1. Overview

The VAST Challenge 2021 is a rerun of the VAST Challenge 2014 with the same story line about the fictitious island country of Kronos, the company GASTech, and the incidents involving missing GAStech employees. However, the data for the VAST Challenge 2021 were modified and new questions were introduced.  

Using the two weeks worth of data leading to the disappearance of the GAStech employees, the goal of [Mini-Challenge 2](https://vast-challenge.github.io/2021/MC2.html) is to analyze the movement and tracking of company vehicles GPS data. Together with GAStech employeeâ€™s credit card transactions and Kronos Kares loyalty card data, the main objectives are:

- Identify anomalies and suspicious behaviours

- Identify which people use which credit card and loyalty cards

# 2. Literature Review

## 2.1 Data Understanding

The 2021 data was compared against 2014 data using the _diffr_ package to better understand the variations of VAST Challenge 2021 from the previous year's challenge. 

- **_car-assignments.csv_** contains vehicle assignments by employee. It lists the employee name, Car ID assignment, employee department, and employee job title.

  + Based on the result of code chunk below, all employee names are the same. However, there are 8 employees with different car ID assignment and employee job title. For example, _Calixto,Nils_ was assigned to _CardID 1_ as _IT Helpdesk_ in 2021 challenge but was previously assigned to _CardID 8_ as _IT Technician_ in 2014 challenge.

```{r diff car, eval = FALSE}
library(diffr)
diffr("data/aspatial/car-assignments.csv", "data_2014/aspatial/car-assignments.csv")
```
![](img/diff_car.png)

- **_loyalty_data.csv_** contains the loyalty card transaction data. It lists the date of the transaction, name of the business, price, and the unique 5-character code loyalty number.

  + The major difference in VAST Challenge 2021 is that loyalty number was used instead of employee names. Other minor differences are the date format ( _line:2_ **0**1/**0**6/2014 vs 1/6/2014) and price of transaction ( example: _line:1145_ Hippokampos,**14.89** vs. Hippokampos,**17.66**).

```{r diff loyal, eval = FALSE}
diffr("data/aspatial/loyalty_data.csv", "data_2014/aspatial/loyalty_data.csv")
```
![](img/diff_loyalty.png)

- **_cc_data.csv_** contains the credit and debit card transaction data. It lists the date and time of the transaction, name of the business, price, and last 4 digits of the credit or debit card number.

  + Similar to the loyalty data, the major difference in that last 4 digits was used instead of employee names. Other minor differences are the date and time format ( _line:2_ **0**1/**0**6/2014 07:28 vs 1/6/2014 7:28) as well as the time and price of transaction ( example: _line:701_ 01/12/2014 **13:17**,Ouzeri Elian,**98.45**  vs. 1/12/2014 **13:03**,Ouzeri Elian,**38.5**).


```{r diff cc, eval = FALSE}
diffr("data/aspatial/cc_data.csv", "data_2014/aspatial/cc_data.csv")
```

![](img/diff_cc.png){width=80%}

- **_gps.csv_** contains the vehicle tracking data. It lists the date and time, car ID, latitude, and longitude coordinates with *685,170* entries. 

  + The file size is too large and it is not practical to do a line by line comparison. Instead of using the _diffr_ , the _md5sum_  from _tools_ package was used to compute the MD5 hashes and detect if there are differences between the 2 files. Based on the result of code chunk below, there are no differences between the 2021 and 2014 gps.csv file.

```{r diff gps}
library(tools)
md5sum("data/aspatial/gps.csv") == md5sum("data_2014/aspatial/gps.csv")
```
- **_MC2-Tourist.jpg_** is the tourist map of Abila, the Capital city of Kronos, with identified location of interest. 

  + Based on visual inspection and _md5sum_ code chunk below, there are no differences between the 2021 and 2014 tourist map.
  
2021 VAST Challenge Map | 2014 VAST Challenge Map
---| ---
![](data/aspatial/MC2-tourist.jpg){width=95%} |![](data_2014/aspatial/MC2-tourist.jpg){width=95%}

```{r diff map}
md5sum("data/aspatial/MC2-tourist.jpg") == md5sum("data_2014/aspatial/MC2-tourist.jpg")
```
- **_ESRI shapefiles_** of Abila and Kronos contains the geospatial vector data format for storing geometric location and associated attribute information.

  + Based on the _md5sum_ code chunk below, there are also no differences between the 2021 and 2014 shapefiles.

```{r diff shapefiles, eval = FALSE}
md5sum("data/Geospatial/Abila.dbf") == md5sum("data_2014/Geospatial/Abila.dbf")
md5sum("data/Geospatial/Abila.kml") == md5sum("data_2014/Geospatial/Abila.kml")
md5sum("data/Geospatial/Abila.prj") == md5sum("data_2014/Geospatial/Abila.prj")
md5sum("data/Geospatial/Abila.sbn") == md5sum("data_2014/Geospatial/Abila.sbn")
md5sum("data/Geospatial/Abila.sbx") == md5sum("data_2014/Geospatial/Abila.sbx")
md5sum("data/Geospatial/Abila.shp") == md5sum("data_2014/Geospatial/Abila.shp")
md5sum("data/Geospatial/Abila.shx") == md5sum("data_2014/Geospatial/Abila.shx")

md5sum("data/Geospatial/Kronos Island.kmz") == md5sum("data_2014/Geospatial/Kronos Island.kmz")
md5sum("data/Geospatial/Kronos_Island.dbf") == md5sum("data_2014/Geospatial/Kronos_Island.dbf")
md5sum("data/Geospatial/Kronos_Island.prj") == md5sum("data_2014/Geospatial/Kronos_Island.prj")
md5sum("data/Geospatial/Kronos_Island.sbn") == md5sum("data_2014/Geospatial/Kronos_Island.sbn")
md5sum("data/Geospatial/Kronos_Island.sbx") == md5sum("data_2014/Geospatial/Kronos_Island.sbx")
md5sum("data/Geospatial/Kronos_Island.shp") == md5sum("data_2014/Geospatial/Kronos_Island.shp")
md5sum("data/Geospatial/Kronos_Island.shx") == md5sum("data_2014/Geospatial/Kronos_Island.shx")
```
## 2.2 Guide Questions

VAST Challenge 2014 focuses about 'Patterns of Life' analysis. It asked about the common daily routines of GAStech employees and what does a day in the life of typical GAStech employee look like.  

In contrast, VAST Challenge 2021 asks to infer the owners of each credit card and loyalty card since the employee names were replaced by  last 4 digits of the credit or debit card number and unique 5-character code loyalty number.

Nevertheless, both challenges want to know about _unusual events_, _anomalies_, and _evidences of suspicious activities_.

## 2.3 Visualisation Approaches

By reviewing the submissions for [VAST Challenge 2014](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/), several approaches were identified to be relevant to the current VAST challenge and reproducible using R data visualisation and data analysis packages.

Mini-Challenge 2 emphasizes the geospatial-temporal data analysis with the financial data from the credit card and loyalty transactions. The common approach from several submissions was to highlight roadway paths of the car and indicate the position and time relationship. The figure below from the [Peking University](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/entries/Peking%20University/), recipient of Excellent Comprehensive Visual Analysis System Award, shows an example geospatial-temporal visualisation.

![](img/litreview_01_peking.png)

This example of movement data visualisation can be achieved using _sf_, _raster_, _readr_, _clock_ and _tmap_ packages. It can also be improved by having an interactive map and tooltip information.

The heatmap visualisation below from [Central South University](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/entries/Central%20South%20University/), recipient of Outstanding Visualization and Analysis Award, shows the credit card transactions of general staff which can also be used to identify the most popular spots and when they are popular. 
![](img/litreview_02_central_south.jpg){width=80%}

This example of heatmap visualisation can be achieved using _gglot2_ and _plotly_ packages. It can also be improved by having an interactive tooltip information.

The concept of 'Point of Interest' (POI) from [Virginia Tech](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/entries/Virginia%20Tech/), recipient of Effective Presentation Honorable Mention, shows the location with a diameter of 50 meters where an employee stops for more than 5 minutes. The POI concept can be utilized to correlate the gps tracking data, credit and debit card transactions, and loyalty card data to help in identifying the owners of the credit card and loyalty cards.

![](img/litreview_03_virginia.jpg)

This example of POI data visualisation can be achieved by reusing the geospatial-temporal packages.

# 3. Methodology

## 3.1 Install and Lauch R Packages

The code chunk below is used to install and load the packages.

```{r r package}
packages = c('ggiraph', 'plotly','lobstr',
             'raster','sf', 'tmap', 
             'igraph', 'tidygraph', 
             'ggraph', 'visNetwork', 
             'lubridate', 'clock',
             'widyr', 'wordcloud',
             'ggwordcloud', 'DT',
             'textplot', 'hms',
             'timetk','tidyverse')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

## 3.2 Import Data

Import the csv files into R using _read_csv()_ of _readr_ package.

```{r import csv}
car_data <- read_csv("data/aspatial/car-assignments.csv")
cc_data <- read_csv("data/aspatial/cc_data.csv")
loyalty_data <- read_csv("data/aspatial/loyalty_data.csv")
gps_data <- read_csv("data/aspatial/gps.csv")

glimpse(car_data)
glimpse(cc_data)
glimpse(loyalty_data)
glimpse(gps_data)
```

Produce a georeference tif file called **abila_map.tif** from the tourist map _MC2-Tourist.jpg_ and _Abila_ shapefiles using an external open-source geographic information system (GIS) software _[QGIS](https://qgis.org/en/site/)_.

![](img/qgis_map.png)

Import _abila_map.tif_ into R using _raster()_ of Raster package, 

```{r bgmap}
bgmap <- raster("data/Geospatial/abila_map.tif")

tm_shape(bgmap) +
tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255)
```

## 3.3 Prepare Data

- **_car_data_**

There are several employees with the same last name and same first name.
Create a new column _FullName_ and combine _FirstName_ and _LastName_ to have unique employee name identifier using _mutate()_ of _dplyr_ package.

Additionally, rename columns _CurrentEmploymentType_ to _Department_ and _CurrentEmploymentTitle_ to _Title_  using _rename()_. Finally, convert _carID_ field from numerical to factor data type.

```{r prep car}
car_data <- car_data %>%
  #concatenate first and last name
  mutate(FullName = paste(FirstName, LastName, sep = " ")) %>%
  rename(Deparment = CurrentEmploymentType) %>%
  rename(Title = CurrentEmploymentTitle)

car_data$CarID <- as_factor(car_data$CarID)

glimpse(car_data)
```

- **_cc_data_**

_Katerina's Cafe'_ causes error when plotting a graph because of special characters.
Convert the special characters into string format using _mutate()_ and _str_detec()_ functions.

Additionally, convert the _timestamp()_ from character datatype to date-time format using _data-time_parse()_ of _clock_ package, then get the date, day of the week, and hour of transaction.

```{r prep cc}

#detect and replace Katerina to Katerina's Cafe
cc_data <- cc_data %>%
    mutate(location = ifelse(str_detect(location, "Katerina"), "Katerina's Cafe", location))

#convert to date-time format
cc_data$date <- date_time_parse(cc_data$timestamp,
                zone = "",
                format = "%m/%d/%Y")
cc_data$day <- wday(cc_data$date,
                          label = TRUE,
                          abbr = TRUE)

cc_data$timestamp <- date_time_parse(cc_data$timestamp,
                zone = "",
                format = "%m/%d/%Y %H:%M")

cc_data$hour <- get_hour(cc_data$timestamp)

glimpse(cc_data)
```

- **_loyalty_data_**

Similar to _cc_data_, convert the special characters of _Katerina's Cafe'_ into string format, convert the _timestamp_ from character datatype to date-time format, then get the date and day of the week of transaction. Note that _loyalty_data_ does not include the hour and minutes of the transaction.

```{r prep loyal}

#detect and replace Katerina to Katerina's Cafe
loyalty_data <- loyalty_data %>%
    mutate(location = ifelse(str_detect(location, "Katerina"), "Katerina's Cafe", location))

#convert to date-time format
loyalty_data$date <- date_time_parse(loyalty_data$timestamp,
                zone = "",
                format = "%m/%d/%Y")

loyalty_data$timestamp <- date_time_parse(loyalty_data$timestamp,
                zone = "",
                format = "%m/%d/%Y")

loyalty_data$day <- wday(loyalty_data$timestamp,
                          label = TRUE,
                          abbr = TRUE)

glimpse(loyalty_data)
```

- **_gps_data_**

Rename _Timestamp_ to _timestamp_ and _id_ to _CarID_ so it will be consistent with other data frame. 
Similar to  _cc_data_ and _loyalty_data_, convert the _timestamp_ from character datatype to date-time format using _data-time_parse()_, then get the date and day of the week.

Convert _CarID_ field from numerical to factor data type. Lastly, convert the gps data frame into a simple feature data frame using _st_as_sf()_ of _sf_ package.

```{r prep gps}

#rename columns for consistency
gps_data <- gps_data %>%
  rename(timestamp = Timestamp) %>%
  rename(CarID = id)

#convert to date-time format
gps_data$date <- date_time_parse(gps_data$timestamp,
                zone = "",
                format = "%m/%d/%Y")

gps_data$day <- as.factor(wday(gps_data$date,
                          label = TRUE,
                          abbr = TRUE))

gps_data$timestamp <- date_time_parse(gps_data$timestamp,
                zone = "",
                format = "%m/%d/%Y %H:%M:%S")

gps_data$hour <- get_hour(gps_data$timestamp)

#convert to factor data type
gps_data$CarID <- as_factor(gps_data$CarID)


glimpse(gps_data)

#convert to simple feature 
gps_sf <- st_as_sf(gps_data, 
                   coords = c("long", "lat"),
                       crs= 4326)
gps_sf
```

## 3.4 Join Data

- **_financial data_**

Combine the _cc_data_ and _loyalty_data_ based on the purchase information like the location, date and price of transaction using _full_join()_ of _dplyr_ package. 
Exclude _day_ and _timestamp_ from _loyalty_data_ since these fields are redundant with _cc_data_. 
Rearrange the columns into _timestamp, date, day, hour, location, price, last4ccnum, loyaltynum_.

```{r join cc_loyal}

#combine based on date, location, price, exclude day and timestamp
cc_loyalty_data <- full_join(cc_data %>% select(-c("day")),
                             loyalty_data %>% select(-c("day","timestamp")), 
                             by = c("date" = "date", 
                                    "location" = "location", 
                                    "price" = "price"))

#get day of the joint data
cc_loyalty_data$day <- wday(cc_loyalty_data$date,
                          label = TRUE,
                          abbr = TRUE)

#rearrange columns
cc_loyalty_data <- cc_loyalty_data %>%
  select("timestamp", "date", "day", "hour", "location", "price", "last4ccnum", "loyaltynum")

glimpse(cc_loyalty_data)
```
The joint financial data reveals 1,807 entries. Some entries have _last4ccnum_ but without _loyaltynum_, have _loyaltynum_ but without _last4ccnum_. 
Additionally, _last4ccnum_ does not necessarily correpond to only 1 _loyaltynum_ which means the owner may use multiple credit or debit cards for their loyalty card or vice versa.

- **_geospatial data_**

Combine the _car_data_ and _gps_data_ based _CarID_ using _left_join()_ of _dplyr_ package. 
Exclude _FirstName_ and _LastName_ from _car_data_ since these fields are redundant with _FullName_. 

```{r join car_gps}

#combine based on CarID
car_gps_data <- left_join(gps_data, 
                          car_data %>% select(-c("FirstName", "LastName")),
                          by = "CarID")

glimpse(car_gps_data)

car_gps_sf <- left_join(gps_sf,
                        car_data %>% select(-c("FirstName", "LastName")),
                        by = "CarID")

car_gps_sf
```
The joint geospatial data reveals that some _CarID_ cannot be mapped to specific employees. Most probably these are the truck drivers who have no specific car assignment.

## 3.5 Proposed Solutions

### Q1: Most Popular Locations

**Using just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies?**

Generate an interactive bar graph in descending order using _ggplot_ and _plotly_ to determine the most popular locations.

```{r q1 popular, fig.height=8}
popular_combine <- cc_loyalty_data %>%
  group_by(location) %>%
  summarize(total_count=n()) %>%
  ggplot(aes(x=reorder(location, total_count),
             y=total_count,
             text = paste("Location :", location,"\n",
                          "Number of transactions:", total_count))) +
  geom_bar(stat="identity", fill = "royalblue4") +
  ggtitle("Combined Credit Card & Loyalty Transactions") +
  labs(x = "Locations", y = "Transaction Count") + 
  coord_flip() +
  theme_minimal()

ggplotly(popular_combine, tooltip = "text")
```
Based on the combined combined credit card and loyalty data, the most popular location is _Katerina's Cafe_ with a total of 256 transactions, followed by _Hippokampos_ with 213 transactions and _Guy's Gyro_ with 187 transactions.


Generate an interactive heatmap using _ggplot_ and _plotly_ to determine the date and time when employees visit the locations.

```{r q1 popular date, fig.height=8}
day_location_count <- cc_loyalty_data %>%
  count(location, day) %>%
  rename(count = n)

popular_day_location <- ggplot(data = day_location_count,
                               aes(x=day, y=reorder(location, desc(location)),
                                   fill = count,
                                   text = paste("Location :", location,"\n",
                                                "Day of week:", day,"\n",
                                                "Number of transactions :", count))) +
  geom_tile()+
  scale_fill_gradient(low = "lightsteelblue1", high = "royalblue4") +
  ggtitle("Combined Credit Card & Loyalty Transactions by Day") +
  labs(x = "Day of the Week",y = "Locations") + 
  theme_minimal()

ggplotly(popular_day_location, tooltip = "text")
```
Based on the combined combined credit card and loyalty data, _Brew've Been Served_ is popular on weekdays, Monday to Friday, with no transactions on weekend.
_Guy's Gyro, Hippokampos, and Katerina's Cafe_ are very popular throughout the week.


```{r q1 popular time, fig.height=8}
hour_location_count <- cc_loyalty_data %>%
  count(location, hour) %>%
  rename(count = n)
  
popular_hour_location <- ggplot(data = hour_location_count,
                               aes(x=hour, y=reorder(location, desc(location)),
                                   fill = count,
                                   text = paste("Location :", location,"\n",
                                                "Hour of the Day:", hour,"\n",
                                                "Number of transactions :", count))) +
  geom_tile()+
  scale_fill_gradient(low = "lightsteelblue1", high = "royalblue4") +
  ggtitle("Combined Credit Card & Loyalty Transactions by Hour of Day") +
  labs(x = "Hour of the Day",y = "Locations") + 
  theme_minimal()

ggplotly(popular_hour_location, tooltip = "text")
```

Based on the time of transaction, _Brew've Been Served and Hallowed Grounds_ are popular in the morning around 7AM and 8AM.

_Abila Zacharo, Bean There Done That, Brewed Awakenings, Gelatogalore, Guy's Gyro, Hippokampos, Jack's Magical Beans, Kalami Kafenion, Katerina's Kafe, and Ouzera Elian_ are popular during lunch break around 12NN to 1PM.

_Guy's Gyro, Hippokampos, and  Katerina's Kafe_ are popular during dinner around 7PM and 8PM.


Generate an interactive boxplot using _plotly_ to determine the outliers and provide clues on some anomalies.

```{r q1 outlier,  fig.height=8}
outlier <- plot_ly(data = cc_loyalty_data,
                   x = ~price,
                   color = I("royalblue4"),
                   alpha = 0.5,
                   boxpoints = "suspectedoutliers") %>%
  add_boxplot(y = ~reorder(location, desc(location))) %>%
  layout(title = "Combined Credit Card & Loyalty Transactions Outliers",
         yaxis = list(title = "Locations"),
         xaxis = list(title = "Price"))

outlier
```

Based on the price of transaction, it seems that there is unusual expensive purchase at _Frydos Autosupply n More_ amount to *10K*.
This is highly suspicious since the mean price for this location is only 161.96.


Generate an interactive linegraph using _plot_anomaly_diagnostics()_ of _plotly_ to diagnose anomalous points in the _cc_data_ purchase prices.
Note that only locations with sufficient number of observations were selected for the anomaly diagnostics.

```{r q1 anomaly,  fig.height=8}

cc_data %>%
  filter(location %in% c("Abila Airport",
                         "Albert's Fine Clothing",
                         "Carlyle Chemical Inc.",
                         "Chostus Hotel",
                         "Frydos Autosupply n' More",
                         "Gelatogalore",
                         "Nationwide Refinery",
                         "Stewart and Sons Fabrication")) %>%
  group_by(location) %>%
  plot_anomaly_diagnostics(timestamp, price, 
                           .facet_ncol = 2,
                           .y_lab = "Price")
```
Based on the anomaly diagnostics, there are unusual purchases in _Gelatogalore, Frydos Autosupply n' More, Albert's Fine Clothing, and Chostus Hotel_.
Again, the most expensive purchase is from _Frydos Autosupply n More_ amounting to **10,000 on 2014-01-13 19:20:00**.

The anomalies will not be removed or corrected. It will be kept in the data as it is since it may lead to more clues in solving the challenge.


### Q2: Anomalies in Vehicle, Credit Card and Loyalty Card Data

**Add the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find?**

Generate an interactive heatmap using _ggplot_ and _plotly_ based on the amount of transactions with missing _last4ccnum_.

```{r q2 missing last4ccnum, fig.height=8}

missing_last4ccnum <- cc_loyalty_data %>%
   filter(is.na(last4ccnum)) 

na_last4ccnum  <- ggplot(data = missing_last4ccnum,
                         aes(x=date, y=reorder(location, desc(location)),
                                   fill = price,
                                   text = paste("Location :", location,"\n",
                                                "Date:", date,"\n",
                                                "Total Amount of Transaction:", price))) +
  geom_tile()+
  scale_fill_gradient(low = "lightsteelblue1", high = "royalblue4") +
  ggtitle("Transactions with Missing Credit Card Data by Date") +
  labs(x = "Date of Transaction", y = "Locations") +
  theme_minimal()

ggplotly(na_last4ccnum, tooltip = "text")
```

Based on the total amount of transactions with missing credit card, _National Refinery_ has a transaction on _2014-01-08_ with a price of _4367.63_.
_Stewart and Sons Fabrication_ has a transaction on _2014-01-13_ with a price of _4071.95_ and another one on _2014-01-15_ with a price of _4485.38_.

The discrepancies may be caused by employees who bought the items from these locations with cash instead of credit card but still used the loyalty card.


Generate another interactive heatmap using _ggplot_ and _plotly_ based on the amount of transactions with missing _loyaltynum_.

```{r q2 missing loyaltynum, fig.height=8}

missing_loyaltynum <- cc_loyalty_data %>%
   filter(is.na(loyaltynum))

na_loyaltynum  <- ggplot(data = missing_loyaltynum,
                         aes(x=date, y=reorder(location, desc(location)),
                                   fill = price,
                                   text = paste("Location :", location,"\n",
                                                "Date:", timestamp,"\n",
                                                "Total Amount of Transaction:", price))) +
  geom_tile()+
  scale_fill_gradient(low = "lightsteelblue1", high = "royalblue4") +
  ggtitle("Transactions with Missing Loyalty Data by Date") +
  labs(x = "Date of Transaction", y = "Locations") +
  theme_minimal()

ggplotly(na_loyaltynum, tooltip = "text")
```

Based on the total amount of transactions with missing loyalty card, _Frydos Autosupply n More_ has a transaction on _2014-01-13 19:20:00_ with a price of _10,000_.

The discrepancy is more suspicious since the person who bought the items did not use his loyalty card which may imply possible misuse of the credit card when making the transaction.


Add the gps and car data by creating a movement path from GPS points using the CarIDs as unique identifier.
Filter the data around the time of transaction from _2014-01-13 19:00 to 21:00_.

```{r q2 gps path 0113}
gps_path_0113 <- car_gps_sf %>%
  filter(timestamp >= "2014-01-13 19:00" & timestamp <= "2014-01-13 21:00") %>%
  group_by(CarID, date) %>%
  summarize(m = mean(timestamp), 
            do_union=FALSE) %>%
  st_cast("LINESTRING") 
```


Plot the gps path on the backgroup tourist map and identity which CarIDs are within the vicinity of _Frydos Autosupply n More_.

```{r q2 gps path 0113 1900}
gps_path_selected_0113 <- gps_path_0113 %>%
  filter(CarID %in% c("13" , "15", "16", "34"))

tmap_mode("view")
tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_path_selected_0113) +
  tm_lines() +
  tm_facets(by = "CarID",  ncol = 1)
```

From the gps paths, CarID _"13" , "15", "16", "34"_ are within the vicinity of _Frydos Autosupply n More_ during suspicions transaction amounting to the price of _10,000_.


Create an interative data table based on the joint gps and car and filter the date to _2014-01-13_.

```{r q2 gps table}
car_gps_0113 <- car_gps_data %>%
  filter(timestamp >= "2014-01-13 19:00" & timestamp <= "2014-01-13 21:00") %>%
  filter(CarID %in% c("13" , "15", "16", "34")) %>%
  group_by(CarID, Deparment, Title, FullName) %>%
  summarise()

DT::datatable(car_gps_0113)
```
From the interactive table, all CarIDs identified are from the Security Department. 
Possibly, _Isia Vann_ and _Edvard Vann_ are relatives because of the same Last Name and working together as Perimeter Controller.


### Q3: Owners of Credit Card and Loyalty Card

**Can you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data?**

It is challenging to infer the owners of the credit card and loyalty card since there is no data field to join the credit card and gps data. 

One possible approach to plot the gps path and check it against timestamp of the credit card transactions. 
This approach assumes that the person driving the car is the same person making the credit card transaction.
Additionally, it assumes that the gps coordinates, timestamp as well as the credit location and timestamp are accurate.

To implement this proposed approach, it assumes that the geospatial tracking software installed in the employees car will stop tracking if the vehicle is not moving.
Borrowing from concept of 'Point of Interest' (POI) from Virginia Tech, POI is identified if the employee stops for more than 5 minutes.

Identify the POIs by computing the difference of gps timestamp. If the difference is greater than 5 minutes, it will be set to poi = TRUE.

```{r q3 poi}
gps_poi_sf <- car_gps_sf %>%
  group_by(CarID) %>%
  mutate(diff = timestamp - lag(timestamp, order_by=CarID)) %>%
  mutate(poi = if_else(diff > 60*5,  TRUE, FALSE)) %>%
  filter(poi == TRUE) %>%
  ungroup() 

glimpse(gps_poi_sf)
```

Plot the POIs on the backgroup tourist map and identity the employee locations.
From the  _Combined Credit Card & Loyalty Transactions by Hour of Day_ heatmap of Question 1, the earliest transaction is around 3:00 AM from Kronos Mart while the last transaction is 10:00 PM from Hippokampos. This information can be used to limit the number of POIs.


```{r q3 poi points}
gps_poi_points <- gps_poi_sf %>%
  filter(hour >= 3 & hour <= 23) %>%
  select(timestamp,
         CarID,
         Deparment,
         Title,
         FullName)

tmap_mode("view")
tm_shape(bgmap) +
  tm_rgb(bgmap, r = 1,g = 2,b = 3,
       alpha = NA,
       saturation = 1,
       interpolate = TRUE,
       max.value = 255) +
  tm_shape(gps_poi_points) +
  tm_dots(col = 'red', border.col = 'black', size = 1, alpha = 0.5, jitter = .8) +
  tm_facets(by = "FullName", ncol = 1)
```

Create an interative data table based on the joint credit card and infer the owner based on the POI plot.

```{r q3 table cc data}

cc_owner <- cc_data %>%
  select(timestamp, location, last4ccnum)

DT::datatable(cc_owner)
```

Finally, manually map the credit card transaction purchases timestamp against the POI map. This approach is very time consuming since it entails manual effort.
As for most cases, the POI map may show certain points around the vicinity of the location but does not have any corresponding credit card purchases.

The owner of the loyalty card will be known after the credit card owner has been identified. 
As noted earlier, there is no one-to-one correspondence between the credit and loyalty card. 
A network map can be used to visualize the number of transactions between specific credit card and loyalty card to know the highest likelihood of the loyalty card owner.

### Q4: Relationships among GASTech Personnel

**Given the data sources provided, identify potential informal or unofficial relationships among GASTech personnel. Provide evidence for these relationships.**

Similar to question 3, identify the POIs by computing the difference of gps timestamp. 
Afterwards, identity who are within 'close contact' of each employee based difference of their gps coordinates within the same time period.
This can help establish the relationship of GASTech personnel based on who are their meeting at the same place and at the same time.

```{r q4 poi network}

gps_poi_network <- car_gps_data %>%
  group_by(CarID) %>%
  mutate(poi_diff = timestamp - lag(timestamp, order_by=CarID)) %>%
  mutate(poi = if_else(poi_diff > 60*5,  TRUE, FALSE)) %>%
  filter(poi == TRUE) %>%
  ungroup() %>%
  mutate(lat_diff = lat - lag(lat, order_by=timestamp))%>%
  mutate(long_diff = long - lag(long, order_by=timestamp)) %>%
  mutate(close_contact = if_else(abs(lat_diff) <=0.001 & abs(long_diff) <=0.001, TRUE, FALSE))%>%
  filter(close_contact == TRUE) %>%
  ungroup()
  
glimpse(gps_poi_network)
```




### Q5: Evidence of Suspicious Activities

**Do you see evidence of suspicious activity? Identify 1- 10 locations where you believe the suspicious activity is occurring, and why.**



# 4. Analysis and Conclusion

# 5. References

* [ISSS608 Assignment Instructions](https://isss608.netlify.app/assignment.html)

* [VAST Challenge 2021: Mini-Challenge 2](https://vast-challenge.github.io/2021/MC2.html)

* [VAST Challenge 2014: MC2 - Patterns of Life Analysis Benchmark Repository](http://visualdata.wustl.edu/varepository/VAST%20Challenge%202014/challenges/MC2%20-%20Patterns%20of%20Life%20Analysis/)

* Whiting, Mark & Cook, Kristin & Grinstein, Georges & Liggett, Kristen & Cooper, Michael & Fallon, John & Morin, Marc. (2014). [VAST challenge 2014: The Kronos incident](http://vis.cs.ucdavis.edu/vis2014papers/VIS_Conference/vast/challenge/whiting.pdf)
